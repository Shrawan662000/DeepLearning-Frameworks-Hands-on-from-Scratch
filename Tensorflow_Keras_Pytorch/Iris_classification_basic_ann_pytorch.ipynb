{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CPU Version of PyTorch\n",
    "If you do not need GPU support, you can install the CPU-only version of PyTorch, which may help avoid issues with missing CUDA-related libraries:\n",
    "\n",
    "##### pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features (4 features)\n",
    "y = iris.target  # Labels (3 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the data (zero mean, unit variance)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4739,  1.2037, -1.5625, -1.3126],\n",
       "        [-0.1331,  2.9924, -1.2760, -1.0456],\n",
       "        [ 1.0859,  0.0857,  0.3859,  0.2892],\n",
       "        [-1.2301,  0.7565, -1.2187, -1.3126],\n",
       "        [-1.7177,  0.3093, -1.3906, -1.3126]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ANN model with 3 hidden layers\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)  # Third hidden layer\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size)  # Output layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 4  # Number of features in the dataset\n",
    "hidden_size1 = 16  # Number of neurons in the first hidden layer\n",
    "hidden_size2 = 12  # Number of neurons in the second hidden layer\n",
    "hidden_size3 = 8  # Number of neurons in the third hidden layer\n",
    "output_size = 3  # Number of output classes (setosa, versicolor, virginica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleANN(\n",
       "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=12, bias=True)\n",
       "  (fc3): Linear(in_features=12, out_features=8, bias=True)\n",
       "  (fc4): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = SimpleANN(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainn the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0230\n",
      "Epoch [20/100], Loss: 0.0198\n",
      "Epoch [30/100], Loss: 0.0164\n",
      "Epoch [40/100], Loss: 0.0132\n",
      "Epoch [50/100], Loss: 0.0104\n",
      "Epoch [60/100], Loss: 0.0083\n",
      "Epoch [70/100], Loss: 0.0065\n",
      "Epoch [80/100], Loss: 0.0051\n",
      "Epoch [90/100], Loss: 0.0041\n",
      "Epoch [100/100], Loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "# Train the model for a specified number of epochs\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: pass the input data through the model to get the predicted outputs\n",
    "    outputs = model(X_train)\n",
    "    # Calculate the loss between the predicted outputs and the actual labels\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization: update the model parameters to minimize the loss\n",
    "    # Zero the gradients of the optimizer to prevent accumulation\n",
    "    optimizer.zero_grad()\n",
    "    # Backpropagate the loss to compute the gradients of the model parameters\n",
    "    loss.backward()\n",
    "    # Update the model parameters using the gradients and the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss at every 10th epoch for monitoring the training progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    # Predict on training and test sets\n",
    "    y_pred_train = model(X_train).argmax(dim=1)\n",
    "    y_pred_test = model(X_test).argmax(dim=1)\n",
    "\n",
    "    # Compute accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f'Training Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "Predicted class for new sample [5.1, 3.5, 1.4, 0.2]: setosa\n"
     ]
    }
   ],
   "source": [
    "# Function for inference on new data\n",
    "def predict_new_data(new_data):\n",
    "    # Standardize the new data (same scaler used for training)\n",
    "    new_data = scaler.transform(np.array(new_data).reshape(1, -1))  # Reshape for single sample\n",
    "    new_data_tensor = torch.tensor(new_data, dtype=torch.float32)  # Convert to tensor\n",
    "    \n",
    "    # Get the model output\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        output = model(new_data_tensor)\n",
    "        print(output.argmax(dim=1))\n",
    "        predicted_class = output.argmax(dim=1).item()  # Get the predicted class\n",
    "\n",
    "    # Return the predicted class\n",
    "    return iris.target_names[predicted_class]  # Return the class name\n",
    "\n",
    "\n",
    "# Example of inference on a new data point\n",
    "new_sample = [5.1, 3.5, 1.4, 0.2]  # A new Iris sample (likely to be Setosa)\n",
    "predicted_class = predict_new_data(new_sample)\n",
    "print(f'Predicted class for new sample {new_sample}: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for new sample [5.7, 2.8, 4.5, 1.3]: versicolor\n",
      "Predicted class for new sample [6.3, 3.3, 6.0, 2.5]: virginica\n",
      "Predicted class for new sample [4.9, 3.0, 1.4, 0.2]: setosa\n",
      "Predicted class for new sample [5.5, 2.4, 3.7, 1.0]: versicolor\n",
      "Predicted class for new sample [7.2, 3.6, 6.1, 2.5]: virginica\n"
     ]
    }
   ],
   "source": [
    "# Function for inference on new data (already defined)\n",
    "def predict_new_data(new_data):\n",
    "    # Standardize the new data (same scaler used for training)\n",
    "    new_data = scaler.transform(np.array(new_data).reshape(1, -1))  # Reshape for single sample\n",
    "    new_data_tensor = torch.tensor(new_data, dtype=torch.float32)  # Convert to tensor\n",
    "    \n",
    "    # Get the model output\n",
    "    with torch.no_grad():  # No need to track gradients for inference\n",
    "        output = model(new_data_tensor)\n",
    "        predicted_class = output.argmax(dim=1).item()  # Get the predicted class\n",
    "\n",
    "    # Return the predicted class\n",
    "    return iris.target_names[predicted_class]  # Return the class name\n",
    "\n",
    "\n",
    "# Example of inference on multiple new data points\n",
    "new_samples = [\n",
    "    [5.7, 2.8, 4.5, 1.3],  # Likely to be Versicolor\n",
    "    [6.3, 3.3, 6.0, 2.5],  # Likely to be Virginica\n",
    "    [4.9, 3.0, 1.4, 0.2],  # Likely to be Setosa\n",
    "    [5.5, 2.4, 3.7, 1.0],  # Likely to be Versicolor\n",
    "    [7.2, 3.6, 6.1, 2.5],  # Likely to be Virginica\n",
    "]\n",
    "\n",
    "# Loop through each new sample and print the predicted class\n",
    "for idx, sample in enumerate(new_samples):\n",
    "    predicted_class = predict_new_data(sample)\n",
    "    print(f'Predicted class for new sample {sample}: {predicted_class}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
