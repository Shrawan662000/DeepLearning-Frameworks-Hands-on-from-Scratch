{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:  5\n",
      "Multiplication:  6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create constants\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "# Perform basic operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "\n",
    "print(\"Addition: \", add.numpy())\n",
    "print(\"Multiplication: \", mul.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Working with Tensors\n",
    "\n",
    "\n",
    "#### 2.1 Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor1:  tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "Tensor2:  tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor from a list\n",
    "tensor1 = tf.constant([1, 2, 3, 4])\n",
    "print(\"Tensor1: \", tensor1)\n",
    "\n",
    "# Creating a tensor with a specific shape\n",
    "tensor2 = tf.constant([[1, 2], [3, 4]])\n",
    "print(\"Tensor2: \", tensor2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor3:  tf.Tensor([2 4 6 8], shape=(4,), dtype=int32)\n",
      "Tensor4:  tf.Tensor(\n",
      "[[ 7 10]\n",
      " [15 22]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Element-wise addition\n",
    "tensor3 = tf.add(tensor1, tensor1)\n",
    "print(\"Tensor3: \", tensor3)\n",
    "\n",
    "# Matrix multiplication\n",
    "tensor4 = tf.matmul(tensor2, tensor2)\n",
    "print(\"Tensor4: \", tensor4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Building Neural Networks\n",
    "\n",
    "#### 3.1 Linear Regression\n",
    "Let's implement a simple linear regression model using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.9965600967407227\n",
      "Epoch 10, Loss: 0.1057320386171341\n",
      "Epoch 20, Loss: 0.0742480680346489\n",
      "Epoch 30, Loss: 0.06007574871182442\n",
      "Epoch 40, Loss: 0.048914361745119095\n",
      "Epoch 50, Loss: 0.04009473696351051\n",
      "Epoch 60, Loss: 0.03312540426850319\n",
      "Epoch 70, Loss: 0.027618229389190674\n",
      "Epoch 80, Loss: 0.023266440257430077\n",
      "Epoch 90, Loss: 0.01982765831053257\n",
      "Trained W: 2.6294147968292236, b: 2.175489902496338\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate some example data\n",
    "x = np.random.rand(100).astype(np.float32)\n",
    "y = 3.0 * x + 2.0 + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Create TensorFlow variables for the model parameters\n",
    "W = tf.Variable(1.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "# Define the linear regression model\n",
    "def linear_regression(x):\n",
    "    return W * x + b\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "def loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Define the training step\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = linear_regression(x)\n",
    "        loss_value = loss(y, y_pred)\n",
    "    gradients = tape.gradient(loss_value, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return loss_value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    loss_value = train_step(x, y)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_value.numpy()}\")\n",
    "\n",
    "print(f\"Trained W: {W.numpy()}, b: {b.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note :\n",
    "1. tf.GradientTape() is a context manager in TensorFlow used for automatic differentiation. \n",
    "\n",
    "2. tape.gradient(loss_value, [W, b])    :    TensorFlow is used to compute the gradients of a specified loss function with respect to a list of variables\n",
    "\n",
    "3. This line updates the model parameters using the computed gradients. The optimizer should be defined elsewhere, typically as an instance of a TensorFlow optimizer (e.g., tf.optimizers.SGD, tf.optimizers.Adam, etc.). The apply_gradients method applies the gradients to the variables to minimize the loss. The zip function pairs each gradient with the corresponding variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Deep Learning with TensorFlow\n",
    "4.1 Building a Simple Neural Network\n",
    "\n",
    "Let's build a simple feedforward neural network for classification using the Keras API, which is integrated into TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.3102 - loss: 1.1155  \n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 734us/step - acc: 0.3862 - loss: 1.0961\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - acc: 0.3659 - loss: 1.0941\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.3611 - loss: 1.0892 \n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71us/step - acc: 0.3838 - loss: 1.0862\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.3910 - loss: 1.0871 \n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - acc: 0.4208 - loss: 1.0786  \n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.4368 - loss: 1.0721\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - acc: 0.4406 - loss: 1.0747 \n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242us/step - acc: 0.4538 - loss: 1.0632\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - acc: 0.4544 - loss: 1.0621\n",
      "Loss: 1.0617644786834717, Accuracy: 0.45899999141693115\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Generate some example data\n",
    "num_samples = 100000\n",
    "num_features = 20\n",
    "num_classes = 3\n",
    "\n",
    "x = np.random.rand(num_samples, num_features)\n",
    "y = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(num_features,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), \n",
    "              loss=SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=10, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x, y)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "Epoch 1/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.2858 - loss: 1.1162  \n",
    "\n",
    "Epoch 2/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step - acc: 0.3486 - loss: 1.0982  \n",
    "\n",
    "Epoch 3/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.3519 - loss: 1.0936 \n",
    "\n",
    "Epoch 4/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 800us/step - acc: 0.3965 - loss: 1.0882\n",
    "\n",
    "Epoch 5/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.3843 - loss: 1.0845\n",
    "\n",
    "Epoch 6/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.3876 - loss: 1.0821 \n",
    "\n",
    "Epoch 7/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 0s/step - acc: 0.4070 - loss: 1.0781  \n",
    "\n",
    "Epoch 8/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.4126 - loss: 1.0775 \n",
    "\n",
    "Epoch 9/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - acc: 0.4235 - loss: 1.0717 \n",
    "\n",
    "Epoch 10/10\n",
    "\n",
    "16/16 ━━━━━━━━━━━━━━━━━━━━ 0s 947us/step - acc: 0.4589 - loss: 1.0667\n",
    "\n",
    "\n",
    "#### Loss: 1.0575286149978638, Accuracy: 0.4659999907016754\n",
    "\n",
    "#### For 1st epoch:\n",
    "\n",
    "16/16: Indicates that the dataset is divided into 16 batches (mini-batches) for training in this epoch.\n",
    "\n",
    "0s 1ms/step: The time taken per step (batch) is approximately 1 millisecond.\n",
    "\n",
    "acc: 0.2858: The accuracy of the model after this epoch is 28.58%.\n",
    "\n",
    "loss: 1.1162: The loss of the model after this epoch is 1.1162.\n",
    "\n",
    "#### Note : here, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note :\n",
    "##### loss=SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "##### Understanding Logits and Probabilities\n",
    "##### Logits:\n",
    " These are raw scores or unnormalized values that a model outputs before applying an activation function like softmax. They can take any real values, and they are not constrained to be between 0 and 1 or to sum up to 1.\n",
    "\n",
    "##### Probabilities: These are normalized values that represent the likelihood of each class, usually obtained by applying a softmax function to the logits. They are constrained to be between 0 and 1 and sum up to 1.\n",
    "\n",
    "#### from_logits Parameter\n",
    "##### from_logits=False:\n",
    "\n",
    "Indicates that the predictions provided to the loss function are probabilities, not logits.\n",
    "In this case, the model’s final layer should be a softmax layer (or another probability-producing layer) that converts logits to probabilities.\n",
    "The loss function will directly use these probabilities to compute the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In TensorFlow and Keras, there are primarily two types of models you can build:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Sequential Model:\n",
    "\n",
    "The Sequential model is a linear stack of layers. It's simple and suitable for most feedforward neural networks where the layers are stacked in a sequence, one after the other.\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s21\\Desktop\\my data\\DeepLearning Frameworks\\Tensorflow\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functional API:\n",
    "\n",
    "The Functional API is more flexible than the Sequential model. It allows you to build complex models such as multi-input, multi-output models, directed acyclic graphs, or models with shared layers.\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "inputs = Input(shape=(784,))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "#### Sequential Model:\n",
    "\n",
    "Pros: Simple to use, suitable for most feedforward networks.\n",
    "\n",
    "Cons: Limited flexibility, cannot handle models with complex architectures (e.g., multi-input, multi-output, layer sharing).\n",
    "\n",
    "#### Functional API:\n",
    "\n",
    "Pros: Highly flexible, can handle complex architectures.\n",
    "\n",
    "Cons: Slightly more complex to set up compared to the Sequential model.\n",
    "\n",
    "### When to Use Each\n",
    "#### Use Sequential Model when:\n",
    "\n",
    "Your model consists of a single input tensor and a single output tensor.\n",
    "\n",
    "The layers are stacked one after the other without any branching or merging.\n",
    "#### Use Functional API when:\n",
    "\n",
    "You need to build models with complex architectures.\n",
    "\n",
    "Your model has multiple inputs or outputs.\n",
    "\n",
    "Layers need to be reused or shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
